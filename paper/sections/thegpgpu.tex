\section*{What is a GPU?}
\addcontentsline{toc}{section}{What is a GPU?}

Graphics Processing Units (GPUs) gained popularity during the mid-1990s. As computers became more prevalent in consumers homes, multimedia functions (games, animations, GUIs) became more demanding on system resources. Looking for a solution, researchers noticed that the types of work being done to draw the screen was unique in that the data set could be broken up into smaller independent data sets. These smaller data sets then had the same operation applied on each one in parallel. The final result was the combination of each data set. 

This style of computation became known as \textbf{S}ingle \textbf{I}nstruction \textbf{M}ultiple \textbf{D}ata (SIMD). The key observation being that each individual operation could be one on each data set in parallel. However, the programability of these operations was limited, initially not extending past transformation and lighting (hardware T$\&L$). 

The evolution of the programmable GPU can be traced with the evolution of Direct3D (commonly known as Microsoft's DirectX). With each evolution in the API, more flexibility was added to the GPU itself. \ref{tab:gpuevolution}

As shown in Table 1, the amount of control a programmer had over the GPU grew with each new iteration of DirectX. In fact, by DirectX 10 with the advent of the unified shader, it was theoretically possible to use the GPU as a floating-point coprocessor. It was at this point, researches started to notice that other computational problems outside of drawing the screen would greatly benefit from the GPUs style of computation. This revelation would change GPU design, even changing its name to the \textbf{G}eneral \textbf{P}urpose graphics processing unit (GPGPU). 

\begin{table}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{DirextX Level} & \textbf{Programmability} \\
		\hline
		$<$ 8.0 & Nothing beyond advanced texture blending \\
		8.0 & Assembly language for vertex processing \\
		9.0 & Data dependent branching and fragment program assembly\\
		10.0 & Unified Vertex/Fragment shaders, Memory Scatter \\
		\hline
	\end{tabular}
	\caption{Evolution of programmable GPUs}
	\label{tab:gpuevolution}
\end{table}


\section*{The birth of the GPGPU}
\addcontentsline{toc}{section}{The birth of the GPGPU}

Unified shaders can be attributed to the single most important moment in the creation of a General Purpose GPU. The unified shader means that the vertex and fragment shaders were combined into one unit. Each GPU would have multiple unified shader units. These units adapted to the needs of the current data set, they could be used as either vertex or fragment shaders varying dynamically during execution. 

Another advantage that the GPU had was its massive number of cores and possible concurrent threads. For programs that could take advantage of the SIMD architecture, could expect to run on a massive amount of hardware. For example, the NVIDIA Geforce 8 architecture, allowed programs to run on 128 processor cores and execute a dizzying 12,228 concurrent threads! GPUs could eclipse the CPU when comparing floating point operations by a huge scale. 

The unified shader represented a more general approach to the CPU freeing programmers from writing their GPGPU programs using the terms of a graphics program. This created an opportunity for GPGPU hardware and programming models to be defined centered around general purpose programming. NVIDIA's Compute Unified Device Architecture (CUDA) was one of the first, allowing programmers to write in a C-like language. 

The ability to write programs in a C-like general language rather than a specialized shader language (that required the programmer to write everything in terms of graphics nomenclature, combined with the increasingly general aspects of the GPU meant that the industry could start porting applications and attempting to figure out what works well on a GPGPU. 

\section*{Unleashing the power of the GPGPU}
\addcontentsline{toc}{section}{Unleashing the power of the GPGPU}


Now that programmers had the tools and APIs they need, programs and experimentation was done to determine what worked well on the GPGPU. 

\subsection*{Linear Algebra}
\addcontentsline{toc}{subsection}{Linear Algebra}

Perhaps one of the most obvious general uses of a GPGPU lays in the realm of liner algebra. It seems fitting since the GPU was designed around matrix math (pixels on a screen). 


\subsection*{Big Data}
\addcontentsline{toc}{subsection}{Big Data}


\subsection*{memcached}
\addcontentsline{toc}{subsection}{memcached}








