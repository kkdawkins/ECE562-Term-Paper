\section*{What is a GPU?}
\addcontentsline{toc}{section}{What is a GPU?}

Graphics Processing Units (GPUs) gained popularity during the mid-1990s. As computers became more prevalent in consumers homes, multimedia functions (games, animations, GUIs) became more demanding on system resources. Looking for a solution, researchers noticed that the types of work being done to draw the screen was unique in that the data set could be broken up into smaller independent data sets. These smaller data sets then had the same operation applied on each one in parallel. The final result was the combination of each data set. 

This style of computation became known as \textbf{S}ingle \textbf{I}nstruction \textbf{M}ultiple \textbf{D}ata (SIMD). The key observation being that each individual operation could be one on each data set in parallel. However, the programability of these operations was limited, initially not extending past transformation and lighting (hardware T$\&L$). 

The evolution of the programmable GPU can be traced with the evolution of Direct3D (commonly known as Microsoft's DirectX). With each evolution in the API, more flexibility was added to the GPU itself. \ref{tab:gpuevolution}

As shown in Table 1, the amount of control a programmer had over the GPU grew with each new iteration of DirectX. In fact, by DirectX 10 with the advent of the unified shader, it was theoretically possible to use the GPU as a floating-point coprocessor. It was at this point, researches started to notice that other computational problems outside of drawing the screen would greatly benefit from the GPUs style of computation. This revelation would change GPU design, even changing its name to the \textbf{G}eneral \textbf{P}urpose graphics processing unit (GPGPU). \cite{emergingtech}

\begin{table}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{DirextX Level} & \textbf{Programmability} \\
		\hline
		$<$ 8.0 & Nothing beyond advanced texture blending \\
		8.0 & Assembly language for vertex processing \\
		9.0 & Data dependent branching and fragment program assembly\\
		10.0 & Unified Vertex/Fragment shaders, Memory Scatter \\
		\hline
	\end{tabular}
	\caption{Evolution of programmable GPUs}
	\label{tab:gpuevolution}
\end{table}


\section*{The birth of the GPGPU}
\addcontentsline{toc}{section}{The birth of the GPGPU}

Unified shaders can be attributed to the single most important moment in the creation of a General Purpose GPU. The unified shader means that the vertex and fragment shaders were combined into one unit. Each GPU would have multiple unified shader units. These units adapted to the needs of the current data set, they could be used as either vertex or fragment shaders varying dynamically during execution. 

Unified shaders also provided one more important attribute to the birth of the GPGPU, a single language to control everything programmable on the GPU. Prior to this, programmers had to know about both the vertex shader language, and the fragment shader language. This was a huge burden to programmers not writing multimedia applications. Unifying the programming language also allowed people to start working on higher level languages for the GPGPU. \textbf{need to cite} 

Another advantage that the GPU had was its massive number of cores and possible concurrent threads. For programs that could take advantage of the SIMD architecture, could expect to run on a massive amount of hardware. For example, the NVIDIA Geforce 8 architecture, allowed programs to run on 128 processor cores and execute a dizzying 12,228 concurrent threads! GPUs could eclipse the CPU when comparing floating point operations by a huge scale. \cite{emergingtech}

The unified shader represented a more general approach to the CPU freeing programmers from writing their GPGPU programs using the terms of a graphics program. This created an opportunity for GPGPU hardware and programming models to be defined centered around general purpose programming. NVIDIA's Compute Unified Device Architecture (CUDA) was one of the first, allowing programmers to write in a C-like language. 

The ability to write programs in a C-like general language rather than a specialized shader language (that required the programmer to write everything in terms of graphics nomenclature, combined with the increasingly general aspects of the GPU meant that the industry could start porting applications and attempting to figure out what works well on a GPGPU. 

\section*{Unleashing the power of the GPGPU}
\addcontentsline{toc}{section}{Unleashing the power of the GPGPU}


Now that programmers had the tools and APIs they need, programs and experimentation was done to determine what worked well on the GPGPU. The following is a sample of real world applications ported to a GPGPU because of their access patterns and potential for performance gains. 

They will be briefly presented here, but the details regarding their porting and lessons learned about GPGPU architecture will not be discussed in detail here. Later in the paper, these topics will be fully discussed and these projects will be referenced. 

\subsection*{Linear Algebra}
\addcontentsline{toc}{subsection}{Linear Algebra}

Perhaps one of the most obvious general uses of a GPGPU lays in the realm of liner algebra. It seems fitting since the GPU was designed around matrix math (pixels on a screen). 

A Cholesky factorization is a common linear algebra operation run on a matrix. By breaking the matrix down into smaller chunks and running the factorization on them, the authors were able to achieve a 3.8X speedup over the standard Intel CPU math library. \cite{linearalg}

\subsection*{Big Data}
\addcontentsline{toc}{subsection}{Big Data}

** I would like to find one of these. But since the paper focuses on the arch, and I have the two others, I may not need it or it would be overkill. Both the memcached and linear papers focus on why the GPU arch is different. 

\subsection*{memcached}
\addcontentsline{toc}{subsection}{memcached}


Memcachd is a distributed memory object cache system. It was designed during the rising popularity of dynamic content on the internet. Memcached stores recently used database information in memory. All of the stored information is combined into massive hash tables. These tables must be crawled efficiently and quickly in order for a performance gain to be attained. The potential problem is that incoming requests are largely dependent on changing data. 

The GPGPU version of memcached ported the \textit{GET} command (after an observation that most requests are \textit{GET} and the write commands may fair better on the CPU. Requests are batched then ran in parallel on the GPGPU running the hash computation and lookup in parallel. Their findings showed that the GPGPU outperformed the CPU by factors reaching 7X speedup. \cite{memcached}


\subsection*{A move into the future}
\addcontentsline{roc}{subsection}{A move into the future}

Combining the ability to perform general purpose computing utilizing common programming languages, and results showing the real world use of GPGPUs the stage has been set for a big revolution in computing. However, if GPGPUs are ever expected to breach into enterprise workloads an integration must take place. All the results showed that while GPGPUs are great for their tasks, they are even better when working closely with the CPU. 

For this change to be meaningful, there must be an efficient line of communication for the CPU and the GPGPU to work together on tasks without a lot of overhead. The current generation of GPGPUs rely on an external bus to bridge the communication and data between the CPU and GPGPU (commonly the PCI-e bus). With data sets growing larger, this is becoming a huge bottleneck that can slow the rest of the system and causes one to constantly wait on the other.

Enter the world of heterogenous computing. Combining a GPGPU directly into the cores of the CPU itself. This has the chance to dramatically increase performance allowing almost seamless transfer whenever appropriate. This does not come without its challenges, and the rest of the paper will explore these in detail, proposing solutions along the way. 












