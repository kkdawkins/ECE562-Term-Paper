\section*{What is a GPU?}
\addcontentsline{toc}{section}{What is a GPU?}

Graphics Processing Units (GPUs) gained popularity during the mid-1990s. As computers became more prevalent in consumer's homes, multimedia functions (games, animations, GUIs) became more demanding on system resources. Looking for a solution, researchers noticed that the types of work being done to draw the screen was unique in that the data set could be broken up into smaller independent data sets. These smaller data sets then had the same operation applied on each one in parallel. The final result was the combination of each data set. 

This style of computation became known as \textbf{S}ingle \textbf{I}nstruction \textbf{M}ultiple \textbf{D}ata (SIMD). The key observation being that each individual operation could be done on each data set in parallel. However, the programability of these operations was limited, initially not extending past transformation and lighting (hardware T$\&L$). 

The evolution of the programmable GPU can be traced with the evolution of Direct3D (commonly known as Microsoft's DirectX). With each evolution in the API, more flexibility was added to the GPU itself. 
 
As shown in Table \ref{tab:gpuevolution}, the amount of control a programmer had over the GPU grew with each new iteration of DirectX. In fact, by DirectX 10 with the advent of the unified shader, it was theoretically possible to use the GPU as a floating-point coprocessor. A great example of this feature begin picked up is the work done at PhysX. While initially an additional expansion card for computing physics, PhysX integrated with GPGPUs capable of DirectX 10. They improved the realism of games by providing a realtime physics environment. It was at this point, researches started to notice that other computational problems outside of drawing the screen would greatly benefit from the GPUs style of computation. This revelation would change GPU design, even changing its name to the \textbf{G}eneral \textbf{P}urpose graphics processing unit (GPGPU). \cite{emergingtech}

\begin{table}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{DirextX Level} & \textbf{Programmability} \\
		\hline
		$<$ 8.0 & Nothing beyond advanced texture blending \\
		8.0 & Assembly language for vertex processing \\
		9.0 & Data dependent branching and fragment program assembly\\
		10.0 & Unified Vertex/Fragment shaders, Memory Scatter \\
		\hline
	\end{tabular}
	\caption{Evolution of programmable GPUs}
	\label{tab:gpuevolution}
\end{table}


\section*{The birth of the GPGPU}
\addcontentsline{toc}{section}{The birth of the GPGPU}

Unified shaders can be attributed to the single most important moment in the creation of a General Purpose GPU. The unified shader means that the vertex and fragment shaders were combined into one unit. Each GPU would have multiple unified shader units. These units adapted to the needs of the current data set, they could be used as either vertex or fragment shaders varying dynamically during execution. 

Unified shaders also provided one more important attribute to the birth of the GPGPU, a single language to control everything programmable on the GPU. Prior to this, programmers had to know about both the vertex shader language, and the fragment shader language. This was a huge burden to programmers not writing multimedia applications. Unifying the programming language also allowed people to start working on higher level languages for the GPGPU. \cite{gpucomputing}

Another advantage that the GPU had was its massive number of cores and possible concurrent threads.  Programs that could take advantage of the SIMD architecture could expect to run on a massive amount of hardware. For example, the NVIDIA Geforce 8 architecture allowed programs to run on 128 processor cores and execute a dizzying 12,228 concurrent threads! GPUs could eclipse the CPU when comparing floating point operations by a huge scale. \cite{emergingtech}

The unified shader represented a more general approach to the CPU, freeing programmers from writing their GPGPU programs using the terms of a graphics program. This created an opportunity for GPGPU hardware and programming models to be defined centered around general purpose programming. NVIDIA's Compute Unified Device Architecture (CUDA) was one of the first, allowing programmers to write in a C-like language. 

The ability to write programs in a C-like general language rather than a specialized shader language (that required the programmer to write everything in terms of graphics nomenclature), combined with the increasingly general aspects of the GPU meant that the industry could start porting applications and attempting to figure out what works well on a GPGPU. 

\section*{Anatomy of the GPGPU}
\addcontentsline{toc}{section}{Anatomy of the GPGPU}

From the top, the GPGPU is made up of several graphics processor clusters (the names vary by manufacturer, but the concept remains the same, this paper standardizes on the Nvidia terminology). The graphics processor clusters contain several streaming multiprocessors which are further divided down into individual cores. The threads are controlled by a threading engine which dispatches threads down through the streaming multiprocessors. \cite{gpuarch}

The GPGPU's cache hierarchy contains a level 1 cache per core with a large level 2 cache shared amongst the graphics processing clusters. The level 1 cache is "used for register spilling, for stack operations, or to improve efficiency of global load and store operations." \cite{gpuarch} The interesting point about the level 2 cache is that in the case of the Nvidia Fermi (GF100) architecture, it contains 2 separate level 2 caches. The first is the standard read/write cache with a write-back replacement policy. The second is a read only for storing texture information for the graphics processor. \cite{gpuarch} 

Typically the GPGPU will contain several memory controllers. In the case of the GF100, it contains six. These are used to support the massive amount of threads and different hardware engines the GPGPU supports. These engines are specialized hardware to support complicated operations (PolyMorph and tessellation, for example). 

The entire anatomy of the GPGPU is built around supporting the massive amount of parallelism that it supplies to the computing system

\section*{Unleashing the power of the GPGPU}
\addcontentsline{toc}{section}{Unleashing the power of the GPGPU}


Now that programmers have the tools and APIs they need, programs and experimentation can be done to determine what worked well on the GPGPU. The following is a sample of real world applications ported to a GPGPU because of their access patterns and potential for performance gains. 

They will be briefly presented here, but the details regarding their porting and lessons learned about GPGPU architecture will not be discussed in detail here. Later in the paper, these topics will be discussed and these projects will be referenced. 

\subsection*{Linear Algebra}
\addcontentsline{toc}{subsection}{Linear Algebra}

Perhaps one of the most obvious general uses of a GPGPU lays in the realm of liner algebra. It seems fitting since the GPU was designed around matrix math (pixels on a screen). 

A Cholesky factorization is a common linear algebra operation run on a matrix. By breaking the matrix down into smaller chunks and running the factorization on them, the authors were able to achieve a 3.8X speedup over the standard Intel CPU math library. \cite{linearalg}

\subsection*{Finance}
\addcontentsline{toc}{subsection}{Finance}

Modeling financial options accurately is one of the biggest challenges on the financial market. If the simulation is not done correctly, then the owner of an asset could sell incorrectly or miss out on another opportunity. The problem is that most of the financial simulations rely on complex data from other simulations. Also, a lot of the computation is done in a form that is known as "path dependent" which means that the solution relies on the path of a price during a specified time period. With the currently available high performance systems the financial market is limited on the number of time steps that they can use before the simulation runs too slow. There are huge opportunities for parallelism in the data making this a great case for a GPGPU. 

This shows that the GPGPU is making its way into even the business market. The efficient calculation of the financial simulations can provide a great service to this market. \cite{finance}

\subsection*{memcached}
\addcontentsline{toc}{subsection}{memcached}


Memcachd is a distributed memory object cache system. It was designed during the rising popularity of dynamic content on the internet. Memcached stores recently used database information in memory. All of the stored information is combined into massive hash tables. These tables must be crawled efficiently and quickly in order for a performance gain to be attained. The potential problem is that incoming requests are largely dependent on changing data. 

The GPGPU version of memcached ported the \textit{GET} command after an observation that most requests are \textit{GET} and the write commands may fair better on the CPU. Requests are batched then ran in parallel on the GPGPU running the hash computation and lookup in parallel. Their findings showed that the GPGPU outperformed the CPU by factors reaching 7X speedup. \cite{memcached}


\subsection*{A move into the future}
\addcontentsline{roc}{subsection}{A move into the future}

Combining the ability to perform general purpose computing utilizing common programming languages, and results showing the real world use of GPGPUs, it has become evident that the stage has been set for a big revolution in computing. However, if GPGPUs are ever expected to breach into enterprise workloads an integration must take place. All the results showed that while GPGPUs are great for their tasks, they are even better when working closely with the CPU. 

For this change to be meaningful, there must be an efficient line of communication for the CPU and the GPGPU to work together on tasks without a lot of overhead. The current generation of GPGPUs rely on an external bus to bridge the communication and data between the CPU and GPGPU (commonly the PCI-e bus). With data sets growing larger, this is becoming a huge bottleneck that can slow the rest of the system and causes one to constantly wait on the other. This also contributes to the large bottleneck on the CPU. Memory controllers are already pushed to the limit to feed multiple cores, in addition to handling the massive amount of data that the GPGPU will need.

Enter the world of heterogenous computing; combining a GPGPU directly into the cores of the CPU itself. This has the chance to dramatically increase performance allowing almost seamless transfer whenever appropriate. This does not come without its challenges, and the rest of the paper will explore these in detail, proposing solutions along the way. 












